{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "\n",
    "# Uncomment if you haven't already installed seaborn\n",
    "# !pip install seaborn\n",
    "\n",
    "\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/processed/master_frame.pkl\", \"rb\") as file:\n",
    "    master_df = pickle.load(file)\n",
    "\n",
    "master_df_clean = master_df.dropna().copy()\n",
    "# Select relevant features for clustering\n",
    "features = master_df_clean[[\"gdp\", \"emp_total_county_naics\", \"emp_occupation\", \"ap\", \"est\", \"gdp_fips\", \"gdp_naics\"]]\n",
    "print(master_df)\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Elbow Method to determine the optimal number of clusters\n",
    "wcss = []  # within-cluster sum of squares\n",
    "\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=i, init=\"k-means++\", max_iter=300, n_init=10, random_state=0\n",
    "    )\n",
    "    kmeans.fit(scaled_features)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow graph\n",
    "plt.plot(range(1, 11), wcss)\n",
    "plt.title(\"Elbow Method\")\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"WCSS\")\n",
    "plt.show()\n",
    "\n",
    "# From the plot we assume that the optimal number of clusters are 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-Means with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=4, init=\"k-means++\", max_iter=300, n_init=10, random_state=0)\n",
    "master_df_clean[\"Cluster\"] = kmeans.fit_predict(scaled_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the clustered data\n",
    "print(master_df_clean)\n",
    "\n",
    "# Select numeric columns for the summary\n",
    "numeric_cols = master_df_clean.select_dtypes(include=[\"number\"]).columns\n",
    "\n",
    "# Summary of clusters\n",
    "cluster_summary = master_df_clean.groupby(\"Cluster\")[numeric_cols].mean()\n",
    "print(cluster_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming master_df_clean and scaled_features are already defined\n",
    "\n",
    "# Apply PCA to reduce to 2 dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=[\"PC1\", \"PC2\"])\n",
    "\n",
    "# Add the cluster assignments to the PCA DataFrame\n",
    "pca_df[\"Cluster\"] = master_df_clean[\"Cluster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(\n",
    "    x=\"PC1\", y=\"PC2\", hue=\"Cluster\", data=pca_df, palette=\"viridis\", s=100, alpha=0.7\n",
    ")\n",
    "plt.title(\"Clusters Visualization with PCA\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Apply PCA to reduce to 2 components\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(scaled_features)\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)  # Adjust eps and min_samples based on your data\n",
    "clusters = dbscan.fit_predict(principal_components)\n",
    "\n",
    "# Create a DataFrame with PCA components and cluster labels\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=[\"PC1\", \"PC2\"])\n",
    "pca_df[\"Cluster\"] = clusters\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(\n",
    "    x=\"PC1\", y=\"PC2\", hue=\"Cluster\", data=pca_df, palette=\"viridis\", s=100, alpha=0.7\n",
    ")\n",
    "plt.title(\"Clusters Visualization with PCA (DBSCAN)\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
